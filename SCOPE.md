# Scope
The GitHub Crawler is a Python script designed to fetch and process repository data from GitHub. It handles rate limiting, converts specific file formats, and logs detailed information about the crawling process. 
## Purpose
To get an overview of interesting organizations and users github repositories.  Currently this is done by just grabbing md, pdf, wikimedia and json files; under the assumption that these are typically enough to determine the general purpose and utilization of a particular repository.  Some contributor data is grabbed as well.
## Goals
- some analytics and statistics on user overlap.  
- fork diff analysis.

## Milestones
